[{"id":1,"title":"Monitoring DreamHack - the World's Largest Digital Festival","author":"apple","author_info":"Software Engineer at MayaData, working on Cloud Native Tech","date":"24-03-2021","tags":"openebs","content":"\nIf you have never heard about [DreamHack](http://www.dreamhack.se/) before, here\nis the pitch: Bring 20,000 people together and have the majority of them bring\ntheir own computer. Mix in professional gaming (eSports), programming contests,\nand live music concerts. The result is the world's largest festival dedicated\nsolely to everything digital.\n\nTo make such an event possible, there needs to be a lot of infrastructure in\nplace. Ordinary infrastructures of this size take months to build, but the crew\nat DreamHack builds everything from scratch in just five days. This of course\nincludes stuff like configuring network switches, but also building the\nelectricity distribution, setting up stores for food and drinks, and even\nbuilding the actual tables.\n\nThe team that builds and operates everything related to the network is\nofficially called the Network team, but we usually refer to ourselves as _tech_\nor _dhtech_. This post is going to focus on the work of dhtech and how we used\nPrometheus during DreamHack Summer 2015 to try to kick our monitoring up another\nnotch.\n\n<!-- more -->\n\n## The equipment\n\nTurns out that to build a highly performant network for 10,000+\ncomputers, you need at least the same number of network ports. In our case these\ncome in the form of ~400 Cisco 2950 switches. We call these the access switches.\nThese are everywhere in the venue where participants will be seated with their\ncomputers.\n\nObviously just connecting all these computers to a switch is not enough. That\nswitch needs to be connected to the other switches as well. This is where the\ndistribution switches (or dist switches) come into play. These are switches that\ntake the hundreds of links from all access switches and aggregate them into\nmore manageable 10-Gbit/s high-capacity fibre. The dist switches are then\nfurther aggregated into our core, where the traffic is routed to its\ndestination.\n\nOn top of all of this, we operate our own WiFi networks, DNS/DHCP servers, and\nother infrastructure. When completed, our core looks something like the image\nbelow.\n\n[![Network planning map](/assets/dh_network_planning_map.png)](/assets/dh_network_planning_map.png)\n\n<center>*The planning map for the distribution and core layers. The core is\nclearly visible in \"Hall D\"*</center>\n\nAll in all this is becoming a lengthy list of stuff to monitor, so let's get to\nthe reason you're here: How do we make sure we know what's going on?\n\n## Introducing: dhmon\n\ndhmon is the collective name of the systems that not only\nmonitor the network, but also allow other teams to collect metrics on whatever\nthey want.\n\nSince the network needs to be built in five days, it's essential that the\nmonitoring systems are easy to set up and keep in sync if we need to do last\nminute infrastructural changes (like adding or removing devices). When we start\nto build the network, we need monitoring as soon as possible to be able to\ndiscover any problems with the equipment or other issues we hadn't foreseen.\n\nIn the past we have tried to use a mix of commonly available software such as\nCacti, SNMPc, and Opsview among others. While these have worked they have focused on\nbeing closed systems and only provided the bare minimum. A few years back a few\npeople from the team said \"Enough, we can do better ourselves!\" and started\nwriting a custom monitoring solution.\n\nAt the time the options were limited. Over the years the system went from using\nGraphite (scalability issues), a custom Cassandra store (high complexity), and\nInfluxDB (immature software) to finally land on using Prometheus. I first\nlearned about Prometheus back in 2014 when I met Julius Volz and I had been\neager to try it ever since. This summer we finally replaced the custom\nInfluxDB-based metrics store that we had written with Prometheus. Spoiler: We're\nnot going back.\n\n## The architecture\n\nThe monitoring solution consists of three layers:\ncollection, storage, presentation. Our most critical collectors are\nsnmpcollector (SNMP) and ipplan-pinger (ICMP), closely followed by dhcpinfo\n(DHCP lease stats). We also have some scripts that dump stats about other\nsystems into [node_exporter](https://github.com/prometheus/node_exporter)'s\ntextfile collector.\n\n[![dhmon Architecture](/assets/dh_dhmon_architecture.png)](/assets/dh_dhmon_architecture.png)\n\n<center>*The current architecture plan of dhmon as of Summer 2015*</center>\n\nWe use Prometheus as a central timeseries storage and querying engine, but we\nalso use Redis and memcached to export snapshot views of binary information that\nwe collect but cannot store in Prometheus in any sensible way, or when we need\nto access very fresh data.\n\nOne such case is in our presentation layer. We use our dhmap web application to\nget an overview of the overall health of the access switches. In order to be\neffective at resolving errors, we need a latency of ~10 seconds from data\ncollection to presentation. Our goal is to have fixed the problem before the\ncustomer notices, or at least before they have walked over to the support people\nto report an issue. For this reason, we have been using memcached since the\nbeginning to access the latest snapshot of the network.\n\nWe continued to use memcached this year for our low-latency data, while using\nPrometheus for everything that's historical or not as latency-sensitive. This\ndecision was made simply because we were unsure how Prometheus would perform at\nvery short sampling intervals. In the end, we found no reason for why we can't\nuse Prometheus for this data as well - we will definitely try to replace our\nmemcached with Prometheus at the next DreamHack.\n\n[![dhmon Visualization](/assets/dh_dhmon_visualization.png)](/assets/dh_dhmon_visualization.png)\n\n<center>*The overview of our access layer visualized by dhmon*</center>\n\n## Prometheus setup\n\nThe block that so far has been referred to as _Prometheus_\nreally consists of three products:\n[Prometheus](https://github.com/prometheus/prometheus),\n[PromDash](https://github.com/prometheus/promdash), and\n[Alertmanager](https://github.com/prometheus/alertmanager). The setup is fairly\nbasic and all three components are running on the same host. Everything is\nserved by an Apache web server that just acts as a reverse proxy.\n\n    ProxyPass /prometheus http://localhost:9090/prometheus\n    ProxyPass /alertmanager http://localhost:9093/alertmanager\n    ProxyPass /dash http://localhost:3000/dash\n\n## Exploring the network\n\nPrometheus has a powerful querying engine that allows\nyou to do pretty cool things with the streaming information collected from all\nover your network. However, sometimes the queries need to process too much data\nto finish within a reasonable amount of time. This happened to us when we wanted\nto graph the top 5 utilized links out of ~18,000 in total. While the query\nworked, it would take roughly the amount of time we set our timeout limit to,\nmeaning it was both slow and flaky. We decided to use Prometheus' [recording\nrules](/docs/prometheus/latest/configuration/recording_rules/) for precomputing heavy queries.\n\n    precomputed_link_utilization_percent = rate(ifHCOutOctets{layer!='access'}[10m])*8/1000/1000\n                                             / on (device,interface,alias)\n                                           ifHighSpeed{layer!='access'}\n\nAfter this, running `topk(5, precomputed_link_utilization_percent)` was\nblazingly fast.\n\n## Being reactive: alerting\n\nSo at this stage we had something we could query for\nthe state of the network. Since we are humans, we don't want to spend our time\nrunning queries all the time to see if things are still running as they should,\nso obviously we need alerting.\n\nFor example: we know that all our access switches use GigabitEthernet0/2 as an\nuplink. Sometimes when the network cables have been in storage for too long they\noxidize and are not able to negotiate the full 1000 Mbps that we want.\n\nThe negotiated speed of a network port can be found in the SNMP OID\n`IF-MIB::ifHighSpeed`. People familiar with SNMP will however recognize that\nthis OID is indexed by an arbitrary interface index. To make any sense of this\nindex, we need to cross-reference it with data from SNMP OID `IF-MIB::ifDescr`\nto retrieve the actual interface name.\n\nFortunately, our snmpcollector supports this kind of cross-referencing while\ngenerating Prometheus metrics. This allows us in a simple way to not only query\ndata, but also define useful alerts. In our setup we configured the SNMP\ncollection to annotate any metric under the `IF-MIB::ifTable` and\n`IF-MIB::ifXTable` OIDs with `ifDescr`. This will come in handy now when we need\nto specify that we are only interested in the `GigabitEthernet0/2` port and no\nother interface.\n\nLet's have a look at what such an alert definition looks like.\n\n    ALERT BadUplinkOnAccessSwitch\n      IF ifHighSpeed{layer='access', interface='GigabitEthernet0/2'} < 1000 FOR 2m\n      SUMMARY \"Interface linking at {{$value}} Mbps\"\n      DESCRIPTION \"Interface {{$labels.interface}} on {{$labels.device}} linking at {{$value}} Mbps\"\n\nDone! Now we will get an alert if a switch's uplink suddenly links at a\nnon-optimal speed.\n\nLet's also look at how an alert for an almost full DHCP scope looks like:\n\n    ALERT DhcpScopeAlmostFull\n      IF ceil((dhcp_leases_current_count / dhcp_leases_max_count)*100) > 90 FOR 2m\n      SUMMARY \"DHCP scope {{$labels.network}} is almost full\"\n      DESCRIPTION \"DHCP scope {{$labels.network}} is {{$value}}% full\"\n\nWe found the syntax to define alerts easy to read and understand even if you had\nno previous experience with Prometheus or time series databases.\n\n[![Prometheus alerts for DreamHack](/assets/dh_prometheus_alerts.png)](/assets/dh_prometheus_alerts.png)\n\n<center>*Oops! Turns out we have some bad uplinks, better run out and fix\nit!*</center>\n\n## Being proactive: dashboards\n\nWhile alerting is an essential part of\nmonitoring, sometimes you just want to have a good overview of the health of\nyour network. To achieve this we used [PromDash](/docs/visualization/promdash/).\nEvery time someone asked us something about the network, we crafted a query to\nget the answer and saved it as a dashboard widget. The most interesting ones\nwere then added to an overview dashboard that we proudly displayed.\n\n[![dhmon Dashboard](/assets/dh_dhmon_dashboard.png)](/assets/dh_dhmon_dashboard.png)\n\n<center>*The DreamHack Overview dashboard powered by PromDash*</center>\n\n## The future\n\nWhile changing an integral part of any system is a complex job and\nwe're happy that we managed to integrate Prometheus in just one event, there are\nwithout a doubt a lot of areas to improve. Some areas are pretty basic: using\nmore precomputed metrics to improve performance, adding more alerts, and tuning\nthe ones we have. Another area is to make it easier for operators: creating an\nalert dashboard suitable for our network operations center (NOC), figuring out\nif we want to page the people on-call, or just let the NOC escalate alerts.\n\nSome bigger features we're planning on adding: syslog analysis (we have a lot of\nsyslog!), alerts from our intrusion detection systems, integrating with our\nPuppet setup, and also integrating more across the different teams at DreamHack.\nWe managed to create a proof-of-concept where we got data from one of the\nelectrical current sensors into our monitoring, making it easy to see if a\ndevice is faulty or if it simply doesn't have any electricity anymore. We're\nalso working on integrating with the point-of-sale systems that are used in the\nstores at the event. Who doesn't want to graph the sales of ice cream?\n\nFinally, not all services that the team operates are on-site, and some even run\n24/7 after the event. We want to monitor these services with Prometheus as well,\nand in the long run when Prometheus gets support for federation, utilize the\noff-site Prometheus to replicate the metrics from the event Prometheus.\n\n## Closing words\n\nWe're really excited about Prometheus and how easy it makes\nsetting up scalable monitoring and alerting from scratch.\n\nA huge shout-out to everyone that helped us in `#prometheus` on\n[FreeNode](https://freenode.net/) during the event. Special thanks to Brian\nBrazil, Fabian Reinartz and Julius Volz. Thanks for helping us even in the cases\nwhere it was obvious that we hadn't read the documentation thoroughly enough.\n\nFinally, dhmon is all open-source, so head over to https://github.com/dhtech/\nand have a look if you're interested. If you feel like you would like to be a\npart of this, just head over to `#dreamhack` on\n[QuakeNet](https://www.quakenet.org/) and have a chat with us. Who knows, maybe\nyou will help us build the next DreamHack?\n4\n\n---\n\n5\n​\n6\n\n# Getting started\n\n7\n​\n8\nThis guide is a \"Hello World\"-style tutorial which shows how to install,\n9\nconfigure, and use a simple Prometheus instance. You will download and run\n10\nPrometheus locally, configure it to scrape itself and an example application,\n11\nthen work with queries, rules, and graphs to use collected time\n12\nseries data.\n13\n​\n14\n\n## Downloading and running Prometheus\n\n15\n​\n16\n[Download the latest release](https://prometheus.io/download) of Prometheus for\n17\nyour platform, then extract and run it:\n18\n​\n19\n\n```bash\n20\ntar xvfz prometheus-*.tar.gz\n21\ncd prometheus-*\n22\n```\n\n23\n​\n24\nBefore starting Prometheus, let's configure it.\n25\n​\n26\n\n## Configuring Prometheus to monitor itself\n\n27\n​\n28\nPrometheus collects metrics from _targets_ by scraping metrics HTTP\n29\nendpoints. Since Prometheus exposes data in the same\n30\nmanner about itself, it can also scrape and monitor its own health.\n31\n​\n32\nWhile a Prometheus server that collects only data about itself is not very\n33\nuseful, it is a good starting example. Save the following basic\n34\nPrometheus configuration as a file named `prometheus.yml`:\n35\n​\n36\n\n```yaml\nglobal:\n  scrape_interval: 15s # By default, scrape targets every 15 seconds.\n\n  # Attach these labels to any time series or alerts when communicating with\n  # external systems (federation, remote storage, Alertmanager).\n  external_labels:\n    monitor: \"codelab-monitor\"\n\n# A scrape configuration containing exactly one endpoint to scrape:\n# Here it's Prometheus itself.\nscrape_configs:\n  # The job name is added as a label `job=<job_name>` to any timeseries scraped from this config.\n  - job_name: \"prometheus\"\n\n    # Override the global default and scrape targets from this job every 5 seconds.\n    scrape_interval: 5s\n\n    static_configs:\n      - targets: [\"localhost:9090\"]\n```\n\nFor a complete specification of configuration options, see the\n[configuration documentation](configuration/configuration.md).\n\n## Starting Prometheus\n\nTo start Prometheus with your newly created configuration file, change to the\ndirectory containing the Prometheus binary and run:\n\n```bash\n# Start Prometheus.\n# By default, Prometheus stores its database in ./data (flag --storage.tsdb.path).\n./prometheus --config.file=prometheus.yml\n```\n\nPrometheus should start up. You should also be able to browse to a status page\nabout itself at [localhost:9090](http://localhost:9090). Give it a couple of\nseconds to collect data about itself from its own HTTP metrics endpoint.\n\nYou can also verify that Prometheus is serving metrics about itself by\nnavigating to its metrics endpoint:\n[localhost:9090/metrics](http://localhost:9090/metrics)\n\n## Using the expression browser\n\nLet us explore data that Prometheus has collected about itself. To\nuse Prometheus's built-in expression browser, navigate to\nhttp://localhost:9090/graph and choose the \"Console\" view within the \"Graph\" tab.\n\nAs you can gather from [localhost:9090/metrics](http://localhost:9090/metrics),\none metric that Prometheus exports about itself is named\n`prometheus_target_interval_length_seconds` (the actual amount of time between\ntarget scrapes). Enter the below into the expression console and then click \"Execute\":\n\n```\nprometheus_target_interval_length_seconds\n```\n\nThis should return a number of different time series (along with the latest value\nrecorded for each), each with the metric name\n`prometheus_target_interval_length_seconds`, but with different labels. These\nlabels designate different latency percentiles and target group intervals.\n\nIf we are interested only in 99th percentile latencies, we could use this\nquery:\n\n```\nprometheus_target_interval_length_seconds{quantile=\"0.99\"}\n```\n\nTo count the number of returned time series, you could write:\n\n```\ncount(prometheus_target_interval_length_seconds)\n```\n\nFor more about the expression language, see the\n[expression language documentation](querying/basics.md).\n\n## Using the graphing interface\n\nTo graph expressions, navigate to http://localhost:9090/graph and use the \"Graph\"\ntab.\n\nFor example, enter the following expression to graph the per-second rate of chunks\nbeing created in the self-scraped Prometheus:\n\n```\nrate(prometheus_tsdb_head_chunks_created_total[1m])\n```\n\nExperiment with the graph range parameters and other settings.\n\n## Starting up some sample targets\n\nLet's add additional targets for Prometheus to scrape.\n\nThe Node Exporter is used as an example target, for more information on using it\n[see these instructions.](https://prometheus.io/docs/guides/node-exporter/)\n\n```bash\ntar -xzvf node_exporter-*.*.tar.gz\ncd node_exporter-*.*\n\n# Start 3 example targets in separate terminals:\n./node_exporter --web.listen-address 127.0.0.1:8080\n./node_exporter --web.listen-address 127.0.0.1:8081\n./node_exporter --web.listen-address 127.0.0.1:8082\n```\n\nYou should now have example targets listening on http://localhost:8080/metrics,\nhttp://localhost:8081/metrics, and http://localhost:8082/metrics.\n\n## Configure Prometheus to monitor the sample targets\n\nNow we will configure Prometheus to scrape these new targets. Let's group all\nthree endpoints into one job called `node`. We will imagine that the\nfirst two endpoints are production targets, while the third one represents a\ncanary instance. To model this in Prometheus, we can add several groups of\nendpoints to a single job, adding extra labels to each group of targets. In\nthis example, we will add the `group=\"production\"` label to the first group of\ntargets, while adding `group=\"canary\"` to the second.\n\nTo achieve this, add the following job definition to the `scrape_configs`\nsection in your `prometheus.yml` and restart your Prometheus instance:\n\n```yaml\nscrape_configs:\n  - job_name: \"node\"\n\n    # Override the global default and scrape targets from this job every 5 seconds.\n    scrape_interval: 5s\n\n    static_configs:\n      - targets: [\"localhost:8080\", \"localhost:8081\"]\n        labels:\n          group: \"production\"\n\n      - targets: [\"localhost:8082\"]\n        labels:\n          group: \"canary\"\n```\n\nGo to the expression browser and verify that Prometheus now has information\nabout time series that these example endpoints expose, such as `node_cpu_seconds_total`.\n\n## Configure rules for aggregating scraped data into new time series\n\nThough not a problem in our example, queries that aggregate over thousands of\ntime series can get slow when computed ad-hoc. To make this more efficient,\nPrometheus can prerecord expressions into new persisted\ntime series via configured _recording rules_. Let's say we are interested in\nrecording the per-second rate of cpu time (`node_cpu_seconds_total`) averaged\nover all cpus per instance (but preserving the `job`, `instance` and `mode`\ndimensions) as measured over a window of 5 minutes. We could write this as:\n\n```\navg by (job, instance, mode) (rate(node_cpu_seconds_total[5m]))\n```\n\nTry graphing this expression.\n\nTo record the time series resulting from this expression into a new metric\ncalled `job_instance_mode:node_cpu_seconds:avg_rate5m`, create a file\nwith the following recording rule and save it as `prometheus.rules.yml`:\n\n```\ngroups:\n- name: cpu-node\n  rules:\n  - record: job_instance_mode:node_cpu_seconds:avg_rate5m\n    expr: avg by (job, instance, mode) (rate(node_cpu_seconds_total[5m]))\n```\n\nTo make Prometheus pick up this new rule, add a `rule_files` statement in your `prometheus.yml`. The config should now\nlook like this:\n\n```yaml\nglobal:\n  scrape_interval: 15s # By default, scrape targets every 15 seconds.\n  evaluation_interval: 15s # Evaluate rules every 15 seconds.\n\n  # Attach these extra labels to all timeseries collected by this Prometheus instance.\n  external_labels:\n    monitor: \"codelab-monitor\"\n\nrule_files:\n  - \"prometheus.rules.yml\"\n\nscrape_configs:\n  - job_name: \"prometheus\"\n\n    # Override the global default and scrape targets from this job every 5 seconds.\n    scrape_interval: 5s\n\n    static_configs:\n      - targets: [\"localhost:9090\"]\n\n  - job_name: \"node\"\n\n    # Override the global default and scrape targets from this job every 5 seconds.\n    scrape_interval: 5s\n\n    static_configs:\n      - targets: [\"localhost:8080\", \"localhost:8081\"]\n        labels:\n          group: \"production\"\n\n      - targets: [\"localhost:8082\"]\n        labels:\n          group: \"canary\"\n```\n\nRestart Prometheus with the new configuration and verify that a new time series\nwith the metric name `job_instance_mode:node_cpu_seconds:avg_rate5m`\nis now available by querying it through the expression browser or graphing it.\n","slug":"monitoring-dreamhack-the-worlds-largest-digital-festival"},{"id":2,"title":"Monitoring DreamHack - the World's Largest Digital Festival","author":"samsung","author_info":"Software Engineer at MayaData, working on Cloud Native Tech","date":"22-03-2021","tags":"tutorials","content":"\n\nOpenEBS knows that you care about how your personal information is used and shared, and we take your privacy seriously. Please read the following to learn more about our privacy policy. By visiting [openebs.io](https://openebs.io) (the \"Site\"), you acknowledge that you accept the practices and policies outlined in this Privacy Policy. For the purposes of the EU General Data Protection Regulation 2016 (the \"GDPR\"), the data controller is MayaData, Inc. whose registered office is at 4300 Stevens Creek Boulevard, Suite 270, San Jose, CA 95129.\n\n### What information do we collect?\n\nPersonal information you provide to us\n\nWe collect personal information from you when you visit our site, request a demo, subscribe to our newsletter or blog, respond to a survey, fill out a form, register for an event or webinar, or otherwise communicate with us or meet with us in person. The information you give us may include your name, employer, position, address, email address, phone number, or areas of interest.\n\nPersonal information we collect\n\nOur web servers collect technical information relating to visitors to our site, including the Internet protocol (IP) address used to connect your computer to the Internet, browser type and version, time zone setting, browser plug-in types and versions, operating system, and platform. We also collect information about your visit, including pages you viewed or searched for, page response times, download errors, length of visits to certain pages, page interaction information (such as scrolling, clicks, and mouse-overs), methods used to browse away from the page, and any phone number used to call our customer service number. This information may be aggregated to measure the number of visits, average time spent on a site, pages viewed, etc. We use this information to measure the use of our site and to improve the content we offer. We may share anonymized experiential information or other data with third parties on an aggregated basis without the use of any information that personally identifies you. We will combine this information with the information you give to us and the information we collect about you. We will use this information and the combined personal information for purposes set out in this Privacy Policy (depending on the types of information we receive).\n\n### Do we use cookies?\n\nYes. Like most websites, we use cookies to understand and save your preferences for future visits, compile aggregate data about site traffic and site interaction so that we can offer better site experiences and tools in the future. For detailed information on the cookies we use and the purposes for which we use them, see our cookie policy.\n\n### \"Do not track\" signals\n\nWe do not respond to web browser \"do not track\" signals. As such, your navigation of our site may be tracked as part of the gathering of quantitative user information described above. If you arrive at our site by way of a link from a third-party site that does respond to \"do not track\" requests, the recognition of any \"do not track\" request you have initiated will end as soon as you reach our site.\n\n- To respond to your questions and provide related customer services;\n- To provide you with information about other goods and services we offer that are similar to those that you have already purchased, provided you have not opted-out of receiving that information;\n- To notify you about changes to our site or other services;\n- To detect and prevent fraud or other financial crime;\n- To monitor and protect the security of our information, systems, and network;\n- For internal business intelligence purposes, product development and enhancement; and\n- To ensure that content from our site is presented most effectively for you and your computer.\n\n### How do we manage email communications?\n\nIf you decide to opt-in to our mailing list, you will receive emails that may include company news, updates, surveys, and related product or service information. If at any time you would like to unsubscribe from receiving future emails, we include an unsubscribe link at the bottom of each email. You can also contact us on the details above. However, there are certain emails that OpenEBS may continue to send per your request or that are necessary for you to receive the full benefit of company services. For example, OpenEBS uses email to deliver information that registered users request as well as provide details about customer account(s), delivery information, and operational information regarding existing products, services, and systems.\n\n### Do we disclose any information to outside parties?\n\nWe do not sell, trade, or otherwise transfer to outside parties your personally identifiable information without your consent. This does not include trusted third parties who assist us in operating our website, conducting our business, or servicing you, so long as those parties agree to keep this information confidential. We may also release your information when we believe release is appropriate to comply with the law, enforce our site policies, or protect ours or others' rights, property, or safety. However, non-personally identifiable visitor information may be provided to other parties for marketing, advertising, or other uses. If we change our privacy policy, we will post changes on this page.\n\n### Children and Privacy\n\nOur site is not directed to children under the age of 13, if you are not 13 years or older, do not use our site. We do not knowingly collect personal information from children under the age of 13. If we learn that personal information of persons less than 13 years-of-age has been collected through our site, we will take the appropriate steps to delete this information.\n\n### Transfer of personal information outside of the European economic area (\"EEA\") and international users\n\nWe are headquartered in the United States. We may access your personal information or transferred to us in the United States or to our affiliates, partners, or service providers who are located worldwide. If you are visiting our site from outside the United States, be aware that your information may be transferred to, stored, and processed in the United States where our servers are located, and our central database is operated. By using our site, you consent to any transfer of this information.\n\nYou have the right to do any of the following with your personal information:\n\n- Request access to your personal information\n- Request correction of your personal information\n- Request erasure of your personal information\n- Object to processing of your personal information\n- Request restriction of processing your personal information\n- Request transfer of your personal information\n- Withdraw your consent for use of personal data\n\nIn addition, where you believe that OpenEBS has not complied with its obligations under this privacy policy or European law, you have the right, in certain circumstances, to make a complaint to an EU data protection authority, such as the UK information commissioner's office. You can seek to exercise any of these rights by contacting us using the details above\n\n### Changes to this privacy policy\n\nAny changes we make to our privacy policy in the future will be posted on this page and, where appropriate, notified to you by email.\n","slug":"monitoring-dreamhack-the-worlds-largest-digital-festival"},{"id":3,"title":"Monitoring DreamHack - the World's Largest Digital Festival","author":"rahul","author_info":"Software Engineer at MayaData, working on Cloud Native Tech","date":"21-03-2021","tags":"devops","content":"\nOpenEBS knows that you care about how your personal information is used and shared, and we take your privacy seriously. Please read the following to learn more about our privacy policy. By visiting [openebs.io](https://openebs.io) (the \"Site\"), you acknowledge that you accept the practices and policies outlined in this Privacy Policy. For the purposes of the EU General Data Protection Regulation 2016 (the \"GDPR\"), the data controller is MayaData, Inc. whose registered office is at 4300 Stevens Creek Boulevard, Suite 270, San Jose, CA 95129.\n\n### What information do we collect?\n\nPersonal information you provide to us\n\nWe collect personal information from you when you visit our site, request a demo, subscribe to our newsletter or blog, respond to a survey, fill out a form, register for an event or webinar, or otherwise communicate with us or meet with us in person. The information you give us may include your name, employer, position, address, email address, phone number, or areas of interest.\n\nPersonal information we collect\n\nOur web servers collect technical information relating to visitors to our site, including the Internet protocol (IP) address used to connect your computer to the Internet, browser type and version, time zone setting, browser plug-in types and versions, operating system, and platform. We also collect information about your visit, including pages you viewed or searched for, page response times, download errors, length of visits to certain pages, page interaction information (such as scrolling, clicks, and mouse-overs), methods used to browse away from the page, and any phone number used to call our customer service number. This information may be aggregated to measure the number of visits, average time spent on a site, pages viewed, etc. We use this information to measure the use of our site and to improve the content we offer. We may share anonymized experiential information or other data with third parties on an aggregated basis without the use of any information that personally identifies you. We will combine this information with the information you give to us and the information we collect about you. We will use this information and the combined personal information for purposes set out in this Privacy Policy (depending on the types of information we receive).\n\n### Do we use cookies?\n\nYes. Like most websites, we use cookies to understand and save your preferences for future visits, compile aggregate data about site traffic and site interaction so that we can offer better site experiences and tools in the future. For detailed information on the cookies we use and the purposes for which we use them, see our cookie policy.\n\n### \"Do not track\" signals\n\nWe do not respond to web browser \"do not track\" signals. As such, your navigation of our site may be tracked as part of the gathering of quantitative user information described above. If you arrive at our site by way of a link from a third-party site that does respond to \"do not track\" requests, the recognition of any \"do not track\" request you have initiated will end as soon as you reach our site.\n\n- To respond to your questions and provide related customer services;\n- To provide you with information about other goods and services we offer that are similar to those that you have already purchased, provided you have not opted-out of receiving that information;\n- To notify you about changes to our site or other services;\n- To detect and prevent fraud or other financial crime;\n- To monitor and protect the security of our information, systems, and network;\n- For internal business intelligence purposes, product development and enhancement; and\n- To ensure that content from our site is presented most effectively for you and your computer.\n\n### How do we manage email communications?\n\nIf you decide to opt-in to our mailing list, you will receive emails that may include company news, updates, surveys, and related product or service information. If at any time you would like to unsubscribe from receiving future emails, we include an unsubscribe link at the bottom of each email. You can also contact us on the details above. However, there are certain emails that OpenEBS may continue to send per your request or that are necessary for you to receive the full benefit of company services. For example, OpenEBS uses email to deliver information that registered users request as well as provide details about customer account(s), delivery information, and operational information regarding existing products, services, and systems.\n\n### Do we disclose any information to outside parties?\n\nWe do not sell, trade, or otherwise transfer to outside parties your personally identifiable information without your consent. This does not include trusted third parties who assist us in operating our website, conducting our business, or servicing you, so long as those parties agree to keep this information confidential. We may also release your information when we believe release is appropriate to comply with the law, enforce our site policies, or protect ours or others' rights, property, or safety. However, non-personally identifiable visitor information may be provided to other parties for marketing, advertising, or other uses. If we change our privacy policy, we will post changes on this page.\n\n### Children and Privacy\n\nOur site is not directed to children under the age of 13, if you are not 13 years or older, do not use our site. We do not knowingly collect personal information from children under the age of 13. If we learn that personal information of persons less than 13 years-of-age has been collected through our site, we will take the appropriate steps to delete this information.\n\n### Transfer of personal information outside of the European economic area (\"EEA\") and international users\n\nWe are headquartered in the United States. We may access your personal information or transferred to us in the United States or to our affiliates, partners, or service providers who are located worldwide. If you are visiting our site from outside the United States, be aware that your information may be transferred to, stored, and processed in the United States where our servers are located, and our central database is operated. By using our site, you consent to any transfer of this information.\n\nYou have the right to do any of the following with your personal information:\n\n- Request access to your personal information\n- Request correction of your personal information\n- Request erasure of your personal information\n- Object to processing of your personal information\n- Request restriction of processing your personal information\n- Request transfer of your personal information\n- Withdraw your consent for use of personal data\n\nIn addition, where you believe that OpenEBS has not complied with its obligations under this privacy policy or European law, you have the right, in certain circumstances, to make a complaint to an EU data protection authority, such as the UK information commissioner's office. You can seek to exercise any of these rights by contacting us using the details above\n\n### Changes to this privacy policy\n\nAny changes we make to our privacy policy in the future will be posted on this page and, where appropriate, notified to you by email.\n","slug":"monitoring-dreamhack-the-worlds-largest-digital-festival"}]
